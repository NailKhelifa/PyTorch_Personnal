{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quelques mots sur les environnements virtuels. \n",
    "\n",
    "**Qu'est-ce-qu'un environnement virtuel ?**\n",
    "\n",
    "\n",
    "* *environnement virtuel* = espace isolé et autonome dans lequel vous pouvez installer des bibliothèques, des packages et des dépendances logicielles spécifiques à un projet sans affecter le système global de votre machine --> utile pour résoudre les problèmes de gestion des dépendances et garantir une isolation entre différents projets.\n",
    "\n",
    "* *idée* = créer un espace de travail indépendant où vous pouvez installer les versions spécifiques de bibliothèques et d'outils nécessaires à votre projet, sans affecter le reste de votre système -> permet d'éviter les conflits de versions et assure que chaque projet peut fonctionner avec les dépendances exactes dont il a besoin.\n",
    "\n",
    "**Avantages de conda dans un environnement virtuel ?**\n",
    "\n",
    "1. *Gestion des dépendances* : Conda gère les dépendances de manière efficace, en installant les packages et en résolvant automatiquement les conflits. Cela simplifie la gestion des environnements et assure la compatibilité entre les packages.\n",
    "\n",
    "2. *Environnements isolés* : Conda permet de créer des environnements virtuels isolés les uns des autres, ce qui signifie que les dépendances d'un projet n'interféreront pas avec celles d'un autre projet.\n",
    "\n",
    "3. *Langages multiples* : Conda n'est pas spécifique à un langage de programmation particulier. Il peut être utilisé pour gérer les environnements virtuels pour des projets Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN, et plus encore.\n",
    "\n",
    "**Voici les commandes utilisées pour créer l'environnement virtuel**\n",
    "\n",
    "- `git clone https://github.com/NailKhelifa/PyTorch_Personnal` : pour cloner ce repertoire\n",
    "\n",
    "- `cd PyTorch_Personnal` : pour se diriger vers ce repertoire\n",
    "\n",
    "- `conda create --name pytorch --file requirements.txt` : pour créer un environnement virtuel du nom de **pytorch**\n",
    "\n",
    "- `conda list --export > requirements.txt` : pour mettre à jour requirements.txt\n",
    "\n",
    "- `conda activate pytorch` : pour activer l'environnement virtuel \n",
    "\n",
    "- `conda list` : montre tous les packages et leur versions dans l'environnement virtuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Les tenseurs sur PyTorch\n",
    "\n",
    "### 1.1. How to create a tensor \n",
    "* In PyTorch, everything is based on tensors operations. They are the equivalent arrays in NumPy. \n",
    "\n",
    "* Tensors can have multiple dimensions (1-D, 2-D, ...). To create an empty tensor use `torch.empty(size)` (for instance `torch.empty(1)` gives a scalar, `torch.empty(3)` gives a 1-D tensor with length 3, `torch.empty(2, 2, 2)` gives a 3-D tensor with length 2).\n",
    "\n",
    "* Randomly initialized tensors with `torch.rand(size)`, tensors with ones `torch.ones(size)`, tensors with zero `torch.zeros(size)`\n",
    "\n",
    "* We can specify the type of values in the tensor with `torch.rand(size, dtype=torch.int)`, `torch.rand(size, dtype=torch.float)`... \n",
    "\n",
    "* We can access the size of the tensor with `x.size` where `x` is a tensor\n",
    "\n",
    "* We can create a tensor using a list `x = torch.tensor([2.5, 3])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. How to make operations on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have element-wise operations using `z = x + y` or `z = torch.add(x, y)`\n",
    "\n",
    "* Inplace operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2570, 0.1259],\n",
       "        [0.4248, 0.5054]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "## element-wise addiiton\n",
    "z = x + y\n",
    "z = torch.add(x, y)\n",
    "y.add_(x) # inplace\n",
    "\n",
    "## element-wise subtraction\n",
    "z = x - y\n",
    "z = torch.subtract(x, y)\n",
    "y.sub_(x) # inplace\n",
    "\n",
    "## element-wise multiplication\n",
    "z = x * y \n",
    "z = torch.mul(x, y)\n",
    "y.mul_(x) # inplace\n",
    "\n",
    "## element-wise division\n",
    "z = x / y\n",
    "z = torch.div(x, y)\n",
    "y.div_(x) # inplace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also slicing operations as with lists and arrays in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8098, 0.3649, 0.3341, 0.4643, 0.4791])\n",
      "0.9479211568832397\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x[:, 0])\n",
    "print(x[1, 1].item()) ## prints the value. Beware -> works only if oyu have one element in your tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 4)\n",
    "y = x.view(16) ## it changes the dimensions --> the number of elements must be the same\n",
    "y = x.view(-1, 8) ## if we dont want to put the array in one dimension, we only specify the \n",
    "                  ## dimension of array and python will determine the rest (here 2 x 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to convert from NumPy array to torch.tensor and the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## from torch to numpy\n",
    "\n",
    "a = torch.ones(6)\n",
    "b = a.numpy() ## be careful with the parenthesis\n",
    " \n",
    "## BE CAREFUL : if you work on the CPU and not the GPU, modifying b will also modify a because \n",
    "##              both objects share the same memory location\n",
    "\n",
    "a.add_(1)\n",
    "print(a, b) ## modifies both objects\n",
    "\n",
    "## from numpy to torch \n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Calculation With Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here focus on the autograd package in pytorch and how we can calculate gradients with it. Gradients are essential for our optimization problems and this is a very useful tool. \n",
    "\n",
    "Pytorch already includes all the tools required to compute gradients with the package autograd.\n",
    "\n",
    "Note the difference: \n",
    "\n",
    "* `torch.randn()`: generates random numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive).\n",
    "\n",
    "* `torch.rand()`: generates random numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive).\n",
    "\n",
    "We want to calculate the gradient of some functions with respect to x. We must specify the argument `requires_grad=True`. Whenever we make some computations with x, PyTorch will create a so-called computational graph for us. For instance, if we say `y = x + 2` PyTorch will create a computational graph such as: \n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "With this graph and the concept of backpropagation, we can compute the gradient of some functions with respect to x. First, in the *forward pass* we calculate the output y and since we specified it requires the gradient, PyTorch will then create automatically a function for us which is later used in the *backpropagation phase* to get the gradients. Here y has an attribute `grad_fn` which will point to a gradient function called `AddBackward0` and with this function we can compute the gradient in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1489, 2.8824, 3.2960], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.0443, 16.6169, 21.7279], grad_fn=<MulBackward0>)\n",
      "tensor(19.5777, grad_fn=<MeanBackward0>)\n",
      "tensor([0.1985, 3.8432, 4.3947])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x + 2 \n",
    "\n",
    "print(y) ## we see the AddBackward0 function\n",
    "\n",
    "z = y * y * 2 \n",
    "\n",
    "print(z) ## MulBackward0 \n",
    "\n",
    "z = z.mean()\n",
    "\n",
    "print(c) ## MeanBackward0\n",
    "\n",
    "## When we want to compute the gradients, we need to call c.backward()\n",
    "\n",
    "z.backward() ## dz/dx -->THIS IS WHERE THERE IS THE VECTOR JACOBIAN PRODUCT, HERE WE DON'T SPECIFY ANY ARGUMENTS BECAUSE Z IS A SCALAR (see print nb 3)\n",
    "print(x.grad) ## we have the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we should also notice is that in the background, what it does is that it creates a so-called **vector jacobian product** that will look like this:\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "We have the jacobian matrix with the partial derivatives and then we multiply it with the gradient vector to get the final gradient that we are interested in. This is called the **chain rule**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0443, 16.6169, 21.7279], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.3176, 26.9027,  4.4211])\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2 ## here z is a vector...\n",
    "\n",
    "print(z)\n",
    "\n",
    "## ...thus, if we are to to the backward operation, we need to multiply it with a vector of same size\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "\n",
    "z.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to modify the `requires_grad` argument and there are three ways to do this (we assume that `x` is a `torch.tensor`)\n",
    "\n",
    "- `x.requires_grad_(False)`: recall that whenever a method ends with a `_`, it modifies the object inplace\n",
    "- `y = x.detach`: this creates a new vector with the same values but it doesn't requires the gradient\n",
    "- `with torch.no_grad(): y = x + 2`: wrap everything in a with sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one last important thing is that whenever we call the `.backward()` function, then the gradient for this tensor will be accumulated in the `.grad` attribute. So the values will be summed up. We need to be careful. Here is a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3): ## we have two iterations, then the second iteration will accumulate the values in the `.grad` attribute\n",
    "                       ## the values are summed up and this is wrong. Thus, before each iteration we must empty the gradient\n",
    "    \n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation\n",
    "\n",
    "In this section, we explain the famous backpropagation algorithm and how we can calculate gradients with it. \n",
    "\n",
    "The first concept we must know is the **chain rule**. Let's say we have two functions a and b. First we have the **input x**, then apply the **function a** which gives us the **output y** and we use this output as the input for our second **function b** which gives us the output **z**.\n",
    "\n",
    "The idea is then to minimize the output **z**. For this, we need to compute the derivative of z with respect to the input x **dz/dx**. We use the so-called **chain rule**.\n",
    "\n",
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next concept is the so-called **computational graph**. For every operation we do with our tensors, PyTorch will create a graph for us where each at each node we apply one operation or one function with some inputs and get some outputs.\n",
    "\n",
    "In this example, we multiply x and y and get a function f. At this node, we can compute so-called **local gradients** which we will use later in the chain rule to compute the final gradient. \n",
    "\n",
    "We can compute two gradients. In this case [f(x, y) = x*y], the local gradients are easy to compute. Why do we want them ? Because at the end of our graph we compute a Loss function that we want to minimize. \n",
    "\n",
    "If we suppose that we know the derivative of our loss function L with respect to the output z denoted **dL/dz**, we can compute our final gradient with the chain rule (see drawing).\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole concept consists of three steps:\n",
    "\n",
    "1. Forward pass: Compute Loss\n",
    "\n",
    "2. Compute local gradients\n",
    "\n",
    "3. Backward pass: Compute dLoss/dWeights using the chain rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Regression\n",
    "\n",
    "We model our output with a linear combination of our input with some weights ($$\\hat{y} = w \\times x$$) and then we formulate our loss function (square error). The idea is to minimize the squared error with respect to our weights. We follow our three steps: \n",
    "\n",
    "1. We compute the loss\n",
    "\n",
    "2. We compute the local gradients at each node of the computational graph\n",
    "\n",
    "3. We start at the end, we have first our derivative of the loss with respect to s and then we go backward with the chain rule.\n",
    "\n",
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with numerical values: \n",
    "\n",
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the value with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0) ## beware, put it float\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1., requires_grad=True) ## BEWARE, WE WANT TO MINIMIZE THE LOSS WITH RESPECT TO THE WEIGHTS SO WE ONLY NEED requires_grad=True FOR W \n",
    "\n",
    "## forward pass and compute loss\n",
    "y_hat = x*w \n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "## backward pas\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#### update weights and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient descent with autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the reader knows how linear regression and gradient descent work and build everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "epoch 21: w = 2.000, loss = 0.00000000\n",
      "epoch 22: w = 2.000, loss = 0.00000000\n",
      "epoch 23: w = 2.000, loss = 0.00000000\n",
      "epoch 24: w = 2.000, loss = 0.00000000\n",
      "epoch 25: w = 2.000, loss = 0.00000000\n",
      "epoch 26: w = 2.000, loss = 0.00000000\n",
      "epoch 27: w = 2.000, loss = 0.00000000\n",
      "epoch 28: w = 2.000, loss = 0.00000000\n",
      "epoch 29: w = 2.000, loss = 0.00000000\n",
      "epoch 30: w = 2.000, loss = 0.00000000\n",
      "epoch 31: w = 2.000, loss = 0.00000000\n",
      "epoch 32: w = 2.000, loss = 0.00000000\n",
      "epoch 33: w = 2.000, loss = 0.00000000\n",
      "epoch 34: w = 2.000, loss = 0.00000000\n",
      "epoch 35: w = 2.000, loss = 0.00000000\n",
      "epoch 36: w = 2.000, loss = 0.00000000\n",
      "epoch 37: w = 2.000, loss = 0.00000000\n",
      "epoch 38: w = 2.000, loss = 0.00000000\n",
      "epoch 39: w = 2.000, loss = 0.00000000\n",
      "epoch 40: w = 2.000, loss = 0.00000000\n",
      "epoch 41: w = 2.000, loss = 0.00000000\n",
      "epoch 42: w = 2.000, loss = 0.00000000\n",
      "epoch 43: w = 2.000, loss = 0.00000000\n",
      "epoch 44: w = 2.000, loss = 0.00000000\n",
      "epoch 45: w = 2.000, loss = 0.00000000\n",
      "epoch 46: w = 2.000, loss = 0.00000000\n",
      "epoch 47: w = 2.000, loss = 0.00000000\n",
      "epoch 48: w = 2.000, loss = 0.00000000\n",
      "epoch 49: w = 2.000, loss = 0.00000000\n",
      "epoch 50: w = 2.000, loss = 0.00000000\n",
      "epoch 51: w = 2.000, loss = 0.00000000\n",
      "epoch 52: w = 2.000, loss = 0.00000000\n",
      "epoch 53: w = 2.000, loss = 0.00000000\n",
      "epoch 54: w = 2.000, loss = 0.00000000\n",
      "epoch 55: w = 2.000, loss = 0.00000000\n",
      "epoch 56: w = 2.000, loss = 0.00000000\n",
      "epoch 57: w = 2.000, loss = 0.00000000\n",
      "epoch 58: w = 2.000, loss = 0.00000000\n",
      "epoch 59: w = 2.000, loss = 0.00000000\n",
      "epoch 60: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 62: w = 2.000, loss = 0.00000000\n",
      "epoch 63: w = 2.000, loss = 0.00000000\n",
      "epoch 64: w = 2.000, loss = 0.00000000\n",
      "epoch 65: w = 2.000, loss = 0.00000000\n",
      "epoch 66: w = 2.000, loss = 0.00000000\n",
      "epoch 67: w = 2.000, loss = 0.00000000\n",
      "epoch 68: w = 2.000, loss = 0.00000000\n",
      "epoch 69: w = 2.000, loss = 0.00000000\n",
      "epoch 70: w = 2.000, loss = 0.00000000\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 72: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 74: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 76: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 78: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 80: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 82: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 84: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 88: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 92: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 94: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 98: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "epoch 100: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32) ## training sample \n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0 \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "## loss\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y - y_predicted) ** 2).mean()\n",
    "\n",
    "## gradient (computed by hand):\n",
    "# MSE = 1/N * (w * x - y) ** 2\n",
    "# dJ/dw = 1/N * 2 * x * (w * x - y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X,Y, y_pred)\n",
    "\n",
    "    # Update weights\n",
    "    w -= learning_rate * dw ## update formula for the gradient descent algorithm\n",
    "\n",
    "    if epoch % 1 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation were we did everything manually. Now we compute the gradient using PyTorch, and we don't resort to numpy anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "## we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "## loss\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y - y_predicted) ** 2).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad ## update formula for the gradient descent algorithm\n",
    "\n",
    "    # Zero gradient\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline: Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we continue the previous example of linear regression but we compute loss and update parameters with PyTorch classes: \n",
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general training pipeline in PyTorch is composed of three steps: \n",
    "\n",
    "1. Design the model (input size, output size, forward pass phase (define all the functions/layers)) \n",
    "\n",
    "2. Design the loss and the optimizer\n",
    "\n",
    "3. Build the training loop: \n",
    "    1. Forward pass : compute the prediction \n",
    "    2. Backward pass : compute the gradients\n",
    "    3. Update the weights and iterate until convergence\n",
    "\n",
    "In the following code, we modified: \n",
    "\n",
    "- `import torch.nn as nn` which gives us the loss function \n",
    "- we don't define a routine for the loss function but instead we use .nn\n",
    "- we don't update manually the weights according to the gradient descent but instead we use the `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn ## neural network module\n",
    "\n",
    "################# STEP 1: design the model, we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "################### STEP 2: we don't define manually the loss anymore\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate) ## stochastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradient\n",
    "    optimizer.zero_grad() ## we still need to empty the gradients\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use completely PyTorch to run the complete pipeline:\n",
    "\n",
    "![Alt text](image-7.png)\n",
    "\n",
    "We now replace our manually implemented forward method with the PyTorch module. Thus we change: \n",
    "\n",
    "- We don't need the weights in the line `w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)` anymore because our PyTorch module knows the parameters\n",
    "\n",
    "- We add `model = nn.Linear()`\n",
    "\n",
    "- We modify the shape of our input and output parameters \n",
    "\n",
    "- We added some data to test our model: `X_test = torch.tensor([5], dtype=torch.float32)`\n",
    "\n",
    "- Modify the optimizer parameters as we removed the weights in our code: `optimizer = torch.optim.SGD(model.parameters, lr=learning_rate) ## stochastic gradient descent`\n",
    "\n",
    "- We defined our prediction by: `y_pred = model(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -2.610\n",
      "epoch 1: w = -0.012, loss = 50.82128143\n",
      "epoch 11: w = 1.622, loss = 1.32085252\n",
      "epoch 21: w = 1.887, loss = 0.03980955\n",
      "epoch 31: w = 1.931, loss = 0.00633789\n",
      "epoch 41: w = 1.939, loss = 0.00516294\n",
      "epoch 51: w = 1.942, loss = 0.00484158\n",
      "epoch 61: w = 1.944, loss = 0.00455924\n",
      "epoch 71: w = 1.946, loss = 0.00429385\n",
      "epoch 81: w = 1.947, loss = 0.00404393\n",
      "epoch 91: w = 1.949, loss = 0.00380856\n",
      "Prediction after training: f(5) = 9.897\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn ## neural network module\n",
    "\n",
    "################# STEP 1: design the model, we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "## model prediction \n",
    "\n",
    "# model = nn.Linear(input_size, output_size) --> this work but we want to create a more general approach using a class\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "################### STEP 2: we don't define manually the loss anymore\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) ## stochastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradient\n",
    "    optimizer.zero_grad() ## we still need to empty the gradients\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
