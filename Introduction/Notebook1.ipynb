{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quelques mots sur les environnements virtuels. \n",
    "\n",
    "**Qu'est-ce-qu'un environnement virtuel ?**\n",
    "\n",
    "\n",
    "* *environnement virtuel* = espace isolé et autonome dans lequel vous pouvez installer des bibliothèques, des packages et des dépendances logicielles spécifiques à un projet sans affecter le système global de votre machine --> utile pour résoudre les problèmes de gestion des dépendances et garantir une isolation entre différents projets.\n",
    "\n",
    "* *idée* = créer un espace de travail indépendant où vous pouvez installer les versions spécifiques de bibliothèques et d'outils nécessaires à votre projet, sans affecter le reste de votre système -> permet d'éviter les conflits de versions et assure que chaque projet peut fonctionner avec les dépendances exactes dont il a besoin.\n",
    "\n",
    "**Avantages de conda dans un environnement virtuel ?**\n",
    "\n",
    "1. *Gestion des dépendances* : Conda gère les dépendances de manière efficace, en installant les packages et en résolvant automatiquement les conflits. Cela simplifie la gestion des environnements et assure la compatibilité entre les packages.\n",
    "\n",
    "2. *Environnements isolés* : Conda permet de créer des environnements virtuels isolés les uns des autres, ce qui signifie que les dépendances d'un projet n'interféreront pas avec celles d'un autre projet.\n",
    "\n",
    "3. *Langages multiples* : Conda n'est pas spécifique à un langage de programmation particulier. Il peut être utilisé pour gérer les environnements virtuels pour des projets Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN, et plus encore.\n",
    "\n",
    "**Voici les commandes utilisées pour créer l'environnement virtuel**\n",
    "\n",
    "- `git clone https://github.com/NailKhelifa/PyTorch_Personnal` : pour cloner ce repertoire\n",
    "\n",
    "- `cd PyTorch_Personnal` : pour se diriger vers ce repertoire\n",
    "\n",
    "- `conda create --name pytorch --file requirements.txt` : pour créer un environnement virtuel du nom de **pytorch**\n",
    "\n",
    "- `conda list --export > requirements.txt` : pour mettre à jour requirements.txt\n",
    "\n",
    "- `conda activate pytorch` : pour activer l'environnement virtuel \n",
    "\n",
    "- `conda list` : montre tous les packages et leur versions dans l'environnement virtuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Les tenseurs sur PyTorch\n",
    "\n",
    "### 1.1. How to create a tensor \n",
    "* In PyTorch, everything is based on tensors operations. They are the equivalent arrays in NumPy. \n",
    "\n",
    "* Tensors can have multiple dimensions (1-D, 2-D, ...). To create an empty tensor use `torch.empty(size)` (for instance `torch.empty(1)` gives a scalar, `torch.empty(3)` gives a 1-D tensor with length 3, `torch.empty(2, 2, 2)` gives a 3-D tensor with length 2).\n",
    "\n",
    "* Randomly initialized tensors with `torch.rand(size)`, tensors with ones `torch.ones(size)`, tensors with zero `torch.zeros(size)`\n",
    "\n",
    "* We can specify the type of values in the tensor with `torch.rand(size, dtype=torch.int)`, `torch.rand(size, dtype=torch.float)`... \n",
    "\n",
    "* We can access the size of the tensor with `x.size` where `x` is a tensor\n",
    "\n",
    "* We can create a tensor using a list `x = torch.tensor([2.5, 3])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. How to make operations on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have element-wise operations using `z = x + y` or `z = torch.add(x, y)`\n",
    "\n",
    "* Inplace operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2570, 0.1259],\n",
       "        [0.4248, 0.5054]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "## element-wise addiiton\n",
    "z = x + y\n",
    "z = torch.add(x, y)\n",
    "y.add_(x) # inplace\n",
    "\n",
    "## element-wise subtraction\n",
    "z = x - y\n",
    "z = torch.subtract(x, y)\n",
    "y.sub_(x) # inplace\n",
    "\n",
    "## element-wise multiplication\n",
    "z = x * y \n",
    "z = torch.mul(x, y)\n",
    "y.mul_(x) # inplace\n",
    "\n",
    "## element-wise division\n",
    "z = x / y\n",
    "z = torch.div(x, y)\n",
    "y.div_(x) # inplace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also slicing operations as with lists and arrays in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8098, 0.3649, 0.3341, 0.4643, 0.4791])\n",
      "0.9479211568832397\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x[:, 0])\n",
    "print(x[1, 1].item()) ## prints the value. Beware -> works only if oyu have one element in your tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 4)\n",
    "y = x.view(16) ## it changes the dimensions --> the number of elements must be the same\n",
    "y = x.view(-1, 8) ## if we dont want to put the array in one dimension, we only specify the \n",
    "                  ## dimension of array and python will determine the rest (here 2 x 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to convert from NumPy array to torch.tensor and the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## from torch to numpy\n",
    "\n",
    "a = torch.ones(6)\n",
    "b = a.numpy() ## be careful with the parenthesis\n",
    " \n",
    "## BE CAREFUL : if you work on the CPU and not the GPU, modifying b will also modify a because \n",
    "##              both objects share the same memory location\n",
    "\n",
    "a.add_(1)\n",
    "print(a, b) ## modifies both objects\n",
    "\n",
    "## from numpy to torch \n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Calculation With Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here focus on the autograd package in pytorch and how we can calculate gradients with it. Gradients are essential for our optimization problems and this is a very useful tool. \n",
    "\n",
    "Pytorch already includes all the tools required to compute gradients with the package autograd.\n",
    "\n",
    "Note the difference: \n",
    "\n",
    "* `torch.randn()`: generates random numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive).\n",
    "\n",
    "* `torch.rand()`: generates random numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive).\n",
    "\n",
    "We want to calculate the gradient of some functions with respect to x. We must specify the argument `requires_grad=True`. Whenever we make some computations with x, PyTorch will create a so-called computational graph for us. For instance, if we say `y = x + 2` PyTorch will create a computational graph such as: \n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "With this graph and the concept of backpropagation, we can compute the gradient of some functions with respect to x. First, in the *forward pass* we calculate the output y and since we specified it requires the gradient, PyTorch will then create automatically a function for us which is later used in the *backpropagation phase* to get the gradients. Here y has an attribute `grad_fn` which will point to a gradient function called `AddBackward0` and with this function we can compute the gradient in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1489, 2.8824, 3.2960], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.0443, 16.6169, 21.7279], grad_fn=<MulBackward0>)\n",
      "tensor(19.5777, grad_fn=<MeanBackward0>)\n",
      "tensor([0.1985, 3.8432, 4.3947])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x + 2 \n",
    "\n",
    "print(y) ## we see the AddBackward0 function\n",
    "\n",
    "z = y * y * 2 \n",
    "\n",
    "print(z) ## MulBackward0 \n",
    "\n",
    "z = z.mean()\n",
    "\n",
    "print(c) ## MeanBackward0\n",
    "\n",
    "## When we want to compute the gradients, we need to call c.backward()\n",
    "\n",
    "z.backward() ## dz/dx -->THIS IS WHERE THERE IS THE VECTOR JACOBIAN PRODUCT, HERE WE DON'T SPECIFY ANY ARGUMENTS BECAUSE Z IS A SCALAR (see print nb 3)\n",
    "print(x.grad) ## we have the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we should also notice is that in the background, what it does is that it creates a so-called **vector jacobian product** that will look like this:\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "We have the jacobian matrix with the partial derivatives and then we multiply it with the gradient vector to get the final gradient that we are interested in. This is called the **chain rule**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0443, 16.6169, 21.7279], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.3176, 26.9027,  4.4211])\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2 ## here z is a vector...\n",
    "\n",
    "print(z)\n",
    "\n",
    "## ...thus, if we are to to the backward operation, we need to multiply it with a vector of same size\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "\n",
    "z.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to modify the `requires_grad` argument and there are three ways to do this (we assume that `x` is a `torch.tensor`)\n",
    "\n",
    "- `x.requires_grad_(False)`: recall that whenever a method ends with a `_`, it modifies the object inplace\n",
    "- `y = x.detach`: this creates a new vector with the same values but it doesn't requires the gradient\n",
    "- `with torch.no_grad(): y = x + 2`: wrap everything in a with sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one last important thing is that whenever we call the `.backward()` function, then the gradient for this tensor will be accumulated in the `.grad` attribute. So the values will be summed up. We need to be careful. Here is a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3): ## we have two iterations, then the second iteration will accumulate the values in the `.grad` attribute\n",
    "                       ## the values are summed up and this is wrong. Thus, before each iteration we must empty the gradient\n",
    "    \n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation\n",
    "\n",
    "In this section, we explain the famous backpropagation algorithm and how we can calculate gradients with it. \n",
    "\n",
    "The first concept we must know is the **chain rule**. Let's say we have two functions a and b. First we have the **input x**, then apply the **function a** which gives us the **output y** and we use this output as the input for our second **function b** which gives us the output **z**.\n",
    "\n",
    "The idea is then to minimize the output **z**. For this, we need to compute the derivative of z with respect to the input x **dz/dx**. We use the so-called **chain rule**.\n",
    "\n",
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next concept is the so-called **computational graph**. For every operation we do with our tensors, PyTorch will create a graph for us where each at each node we apply one operation or one function with some inputs and get some outputs.\n",
    "\n",
    "In this example, we multiply x and y and get a function f. At this node, we can compute so-called **local gradients** which we will use later in the chain rule to compute the final gradient. \n",
    "\n",
    "We can compute two gradients. In this case [f(x, y) = x*y], the local gradients are easy to compute. Why do we want them ? Because at the end of our graph we compute a Loss function that we want to minimize. \n",
    "\n",
    "If we suppose that we know the derivative of our loss function L with respect to the output z denoted **dL/dz**, we can compute our final gradient with the chain rule (see drawing).\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole concept consists of three steps:\n",
    "\n",
    "1. Forward pass: Compute Loss\n",
    "\n",
    "2. Compute local gradients\n",
    "\n",
    "3. Backward pass: Compute dLoss/dWeights using the chain rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Regression\n",
    "\n",
    "We model our output with a linear combination of our input with some weights ($$\\hat{y} = w \\times x$$) and then we formulate our loss function (square error). The idea is to minimize the squared error with respect to our weights. We follow our three steps: \n",
    "\n",
    "1. We compute the loss\n",
    "\n",
    "2. We compute the local gradients at each node of the computational graph\n",
    "\n",
    "3. We start at the end, we have first our derivative of the loss with respect to s and then we go backward with the chain rule.\n",
    "\n",
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with numerical values: \n",
    "\n",
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the value with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0) ## beware, put it float\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1., requires_grad=True) ## BEWARE, WE WANT TO MINIMIZE THE LOSS WITH RESPECT TO THE WEIGHTS SO WE ONLY NEED requires_grad=True FOR W \n",
    "\n",
    "## forward pass and compute loss\n",
    "y_hat = x*w \n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "## backward pas\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#### update weights and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient descent with autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the reader knows how linear regression and gradient descent work and build everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "epoch 21: w = 2.000, loss = 0.00000000\n",
      "epoch 22: w = 2.000, loss = 0.00000000\n",
      "epoch 23: w = 2.000, loss = 0.00000000\n",
      "epoch 24: w = 2.000, loss = 0.00000000\n",
      "epoch 25: w = 2.000, loss = 0.00000000\n",
      "epoch 26: w = 2.000, loss = 0.00000000\n",
      "epoch 27: w = 2.000, loss = 0.00000000\n",
      "epoch 28: w = 2.000, loss = 0.00000000\n",
      "epoch 29: w = 2.000, loss = 0.00000000\n",
      "epoch 30: w = 2.000, loss = 0.00000000\n",
      "epoch 31: w = 2.000, loss = 0.00000000\n",
      "epoch 32: w = 2.000, loss = 0.00000000\n",
      "epoch 33: w = 2.000, loss = 0.00000000\n",
      "epoch 34: w = 2.000, loss = 0.00000000\n",
      "epoch 35: w = 2.000, loss = 0.00000000\n",
      "epoch 36: w = 2.000, loss = 0.00000000\n",
      "epoch 37: w = 2.000, loss = 0.00000000\n",
      "epoch 38: w = 2.000, loss = 0.00000000\n",
      "epoch 39: w = 2.000, loss = 0.00000000\n",
      "epoch 40: w = 2.000, loss = 0.00000000\n",
      "epoch 41: w = 2.000, loss = 0.00000000\n",
      "epoch 42: w = 2.000, loss = 0.00000000\n",
      "epoch 43: w = 2.000, loss = 0.00000000\n",
      "epoch 44: w = 2.000, loss = 0.00000000\n",
      "epoch 45: w = 2.000, loss = 0.00000000\n",
      "epoch 46: w = 2.000, loss = 0.00000000\n",
      "epoch 47: w = 2.000, loss = 0.00000000\n",
      "epoch 48: w = 2.000, loss = 0.00000000\n",
      "epoch 49: w = 2.000, loss = 0.00000000\n",
      "epoch 50: w = 2.000, loss = 0.00000000\n",
      "epoch 51: w = 2.000, loss = 0.00000000\n",
      "epoch 52: w = 2.000, loss = 0.00000000\n",
      "epoch 53: w = 2.000, loss = 0.00000000\n",
      "epoch 54: w = 2.000, loss = 0.00000000\n",
      "epoch 55: w = 2.000, loss = 0.00000000\n",
      "epoch 56: w = 2.000, loss = 0.00000000\n",
      "epoch 57: w = 2.000, loss = 0.00000000\n",
      "epoch 58: w = 2.000, loss = 0.00000000\n",
      "epoch 59: w = 2.000, loss = 0.00000000\n",
      "epoch 60: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 62: w = 2.000, loss = 0.00000000\n",
      "epoch 63: w = 2.000, loss = 0.00000000\n",
      "epoch 64: w = 2.000, loss = 0.00000000\n",
      "epoch 65: w = 2.000, loss = 0.00000000\n",
      "epoch 66: w = 2.000, loss = 0.00000000\n",
      "epoch 67: w = 2.000, loss = 0.00000000\n",
      "epoch 68: w = 2.000, loss = 0.00000000\n",
      "epoch 69: w = 2.000, loss = 0.00000000\n",
      "epoch 70: w = 2.000, loss = 0.00000000\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 72: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 74: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 76: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 78: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 80: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 82: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 84: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 86: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 88: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 90: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 92: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 94: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 96: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 98: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "epoch 100: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32) ## training sample \n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0 \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "## loss\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y - y_predicted) ** 2).mean()\n",
    "\n",
    "## gradient (computed by hand):\n",
    "# MSE = 1/N * (w * x - y) ** 2\n",
    "# dJ/dw = 1/N * 2 * x * (w * x - y)\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients\n",
    "    dw = gradient(X,Y, y_pred)\n",
    "\n",
    "    # Update weights\n",
    "    w -= learning_rate * dw ## update formula for the gradient descent algorithm\n",
    "\n",
    "    if epoch % 1 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation were we did everything manually. Now we compute the gradient using PyTorch, and we don't resort to numpy anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "## we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "## loss\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y - y_predicted) ** 2).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad ## update formula for the gradient descent algorithm\n",
    "\n",
    "    # Zero gradient\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline: Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we continue the previous example of linear regression but we compute loss and update parameters with PyTorch classes: \n",
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general training pipeline in PyTorch is composed of three steps: \n",
    "\n",
    "1. Design the model (input size, output size, forward pass phase (define all the functions/layers)) \n",
    "\n",
    "2. Design the loss and the optimizer\n",
    "\n",
    "3. Build the training loop: \n",
    "    1. Forward pass : compute the prediction \n",
    "    2. Backward pass : compute the gradients\n",
    "    3. Update the weights and iterate until convergence\n",
    "\n",
    "In the following code, we modified: \n",
    "\n",
    "- `import torch.nn as nn` which gives us the loss function \n",
    "- we don't define a routine for the loss function but instead we use .nn\n",
    "- we don't update manually the weights according to the gradient descent but instead we use the `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn ## neural network module\n",
    "\n",
    "################# STEP 1: design the model, we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) \n",
    "\n",
    "## model prediction \n",
    "\n",
    "def forward(x): \n",
    "    return w * x\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "################### STEP 2: we don't define manually the loss anymore\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate) ## stochastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradient\n",
    "    optimizer.zero_grad() ## we still need to empty the gradients\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use completely PyTorch to run the complete pipeline:\n",
    "\n",
    "![Alt text](image-7.png)\n",
    "\n",
    "We now replace our manually implemented forward method with the PyTorch module. Thus we change: \n",
    "\n",
    "- We don't need the weights in the line `w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)` anymore because our PyTorch module knows the parameters\n",
    "\n",
    "- We add `model = nn.Linear()`\n",
    "\n",
    "- We modify the shape of our input and output parameters \n",
    "\n",
    "- We added some data to test our model: `X_test = torch.tensor([5], dtype=torch.float32)`\n",
    "\n",
    "- Modify the optimizer parameters as we removed the weights in our code: `optimizer = torch.optim.SGD(model.parameters, lr=learning_rate) ## stochastic gradient descent`\n",
    "\n",
    "- We defined our prediction by: `y_pred = model(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -2.610\n",
      "epoch 1: w = -0.012, loss = 50.82128143\n",
      "epoch 11: w = 1.622, loss = 1.32085252\n",
      "epoch 21: w = 1.887, loss = 0.03980955\n",
      "epoch 31: w = 1.931, loss = 0.00633789\n",
      "epoch 41: w = 1.939, loss = 0.00516294\n",
      "epoch 51: w = 1.942, loss = 0.00484158\n",
      "epoch 61: w = 1.944, loss = 0.00455924\n",
      "epoch 71: w = 1.946, loss = 0.00429385\n",
      "epoch 81: w = 1.947, loss = 0.00404393\n",
      "epoch 91: w = 1.949, loss = 0.00380856\n",
      "Prediction after training: f(5) = 9.897\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn ## neural network module\n",
    "\n",
    "################# STEP 1: design the model, we assume the model is f(x) = 2 * x, i.e. the weights are here given by 2 \n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) ## training sample \n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "## model prediction \n",
    "\n",
    "# model = nn.Linear(input_size, output_size) --> this work but we want to create a more general approach using a class\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "################### STEP 2: we don't define manually the loss anymore\n",
    "\n",
    "## training: \n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) ## stochastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero gradient\n",
    "    optimizer.zero_grad() ## we still need to empty the gradients\n",
    "\n",
    "    if epoch % 10 == 0: ## we print every step\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In this section we construct the whole linear regression model from scratch using python. \n",
    "\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop: \n",
    "- forward pass: compute prediction loss\n",
    "- backward pass: gradients\n",
    "- update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp312-cp312-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.49.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/khelifanail/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/khelifanail/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/khelifanail/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/khelifanail/anaconda3/envs/pytorch/lib/python3.12/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/khelifanail/anaconda3/envs/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp312-cp312-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp312-cp312-macosx_10_9_x86_64.whl (259 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.49.0-cp312-cp312-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp312-cp312-macosx_10_9_x86_64.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 kiwisolver-1.4.5 matplotlib-3.8.3 pyparsing-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss= 4343.0381\n",
      "epoch: 20, loss= 3240.5144\n",
      "epoch: 30, loss= 2443.0042\n",
      "epoch: 40, loss= 1865.4977\n",
      "epoch: 50, loss= 1446.8823\n",
      "epoch: 60, loss= 1143.1592\n",
      "epoch: 70, loss= 922.6050\n",
      "epoch: 80, loss= 762.3188\n",
      "epoch: 90, loss= 645.7469\n",
      "epoch: 100, loss= 560.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x139f2c9b0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 0) prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1) ## we want to reshape our y because it has one row and we want to make it a column vector : \n",
    "\n",
    "## view is a built-in PyTorch method to reshape tensors\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 1) model : just one layer\n",
    "input_size = n_features\n",
    "output_size = 1 ## we just want one value for the prediction we make\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) optimizer and loss\n",
    "criterion = nn.MSELoss() ## built-in optimization method\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) ## the arguments are the parameters we want to optimize and a learning rate\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    ## we follow the three steps \n",
    "\n",
    "    # forward-pass: \n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # we have to empty our gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch + 1 }, loss= {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Plot\n",
    "        \n",
    "predicted = model(X).detach() ##.detach() generates a new tensor that sets the required_gradient algorithm to zero\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Logistic regression\n",
    "\n",
    "1. Design model (input, output size, forward pass)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop: \n",
    "- forward pass: compute prediction loss\n",
    "- backward pass: gradients\n",
    "- update weights\n",
    "\n",
    "Here is the binary cross entropy loss function: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{BCE} = \\frac{1}{N} \\sum_{i=1}^n (Y_i \\cdot \\log(Y_i) + (1 - Y_i) \\cdot \\log(1-\\hat{Y}_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10, loss = 0.2491\n",
      "epoch : 20, loss = 0.1773\n",
      "epoch : 30, loss = 0.1471\n",
      "epoch : 40, loss = 0.1295\n",
      "epoch : 50, loss = 0.1178\n",
      "epoch : 60, loss = 0.1093\n",
      "epoch : 70, loss = 0.1027\n",
      "epoch : 80, loss = 0.0974\n",
      "epoch : 90, loss = 0.0931\n",
      "epoch : 100, loss = 0.0895\n",
      "accuracy = 0.9298\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # separating between train and test split\n",
    "\n",
    "# 0. Prepare the data\n",
    "\n",
    "bc = datasets.load_breast_cancer() ## binary classification problem\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "#scale \n",
    "sc = StandardScaler() # that will make our features have 0 mean and unit variance, THIS IS ALWAYS RECOMMENDED TO DO WHEN WE DEAL WITH LOGISTIC REGRESSION\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#convert the data to torch.tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1) # we want to make it a column vector\n",
    "y_test = y_test.view(y_test.shape[0], 1) # we want to make it a column vector\n",
    "\n",
    "# 1. model: f=wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1) ## we create one layer using nn.Linear --> we want 1 output and give an input of size n_input \n",
    "\n",
    "    def forward(self, data):\n",
    "        y_predicted = torch.sigmoid(self.linear(data))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. Loss and optimizer\n",
    "learning_rate = 0.1\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward() ## PyTorch does all the computations for us\n",
    "\n",
    "    # updates \n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients (because the .backward()) adds the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 ==  0:\n",
    "         print(f'epoch : {epoch + 1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset and DataLoader - Batch training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Softmax and Cross Entropy\n",
    "\n",
    "These are one of the most common functions used for neural networks. \n",
    "\n",
    "The formula of the softmax is the following: \n",
    "\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_{i=1}^n e^{y_i}}\n",
    "$$\n",
    "\n",
    "This function squashes the output to be between 0 and 1 so that we get probabilities. Let's take an example of a neural network of linear layer with three output values. These values are so-called **scores** or **logits** which are raw and after the softmax function, we get probabilities. \n",
    "\n",
    "![Alt text](image-9.png)\n",
    "\n",
    "The highest logit has the highest probability. Here is the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of time, the softmax function is combined with the so-called **cross-entropy loss** function. This matress the performance of our classification model whose output is a probability between 0 and 1. The loss increases as the predicted probability diverges from the actual label. The better is our prediction, the lower is the loss. \n",
    "\n",
    "It is expressed as: \n",
    "\n",
    "$$\n",
    "\\mathcal{D}(L, \\hat{L}) = - \\frac{1}{N} \\sum_{i=1}^n Y_i \\cdot \\log \\hat{Y}_i\n",
    "$$\n",
    "Here is an example with a good prediction and a bad prediction:\n",
    "\n",
    "![Alt text](image-10.png)\n",
    "\n",
    "Here is the PyTorch code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41154685616493225\n",
      "2.3508665561676025\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "## In PyTorch, the loss function allows for multiple samples --> we can take 3 possible classes\n",
    "y = torch.tensor([2, 0, 1]) ## 3 classes\n",
    "# size : n_samples x n_classes\n",
    "Y_pred_good = torch.tensor([ [0.5, 1.0, 2.0], [2.0, 0.7, 0.1], [0.7, 2.0, 0.6] ])\n",
    "Y_pred_bad = torch.tensor([ [3.0, 1.0, 0.8], [0.8, 0.7, 2.], [1.0, 0.2, 3.0] ])\n",
    "\n",
    "l1 = loss(Y_pred_good, y)\n",
    "l2 = loss(Y_pred_bad, y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item()) \n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(predictions1)\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a typical neural network looks like. Here is a simple neural network in a **multi-class** classification proble. Here we want to find out what animal the image shows. \n",
    "\n",
    "We have an input layers, then some hidden layers and maybe some **activation function** in between. Then at the end we have a **linear layer** with one output for each class. Here we have two outputs and then at the very end we apply our softmax and get the probabilities. \n",
    "\n",
    "![Alt text](image-12.png)\n",
    "\n",
    "Let's see how the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "## Multi-class problem\n",
    "\n",
    "class NeuralNet2(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end because it is already coded in the cross-entropy loss function \n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change slightly the architecture, we get:\n",
    "\n",
    "![Alt text](image-13.png)\n",
    "\n",
    "Notice that we use the sigmoid function here instead of the softmax !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "## Multi-class problem\n",
    "\n",
    "class NeuralNet2(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x): ## during this phase, we compute the prediction \n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end because it is already coded in the cross-entropy loss function \n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss() # applies softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Activation function \n",
    "\n",
    "- What are activation functions?\n",
    "- Why they are used?\n",
    "- What different types of function there are\n",
    "- How do we incorporate them in our PyTorch model?\n",
    "\n",
    "\n",
    "**Activation function apply a non-linear transformation and decide whether a neuron should be activated or not**. Why do we use them? Why is a linear transformation not good enough? If we just apply linear transformations to our inputs, without activation functions, our network is just a stacked linear regression model. This linear model is not suited for a more complex task. With non-linear transformations in-between, our network can learn better and perform more complex tasks. After each layer, we typically want to apply this activation function so that our model can learn better.\n",
    "\n",
    "![Alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of classic activation functions:\n",
    "- Step function: \n",
    "\n",
    "$$\n",
    "f(x) =  \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    1 & \\text{if   } x \\geq \\theta \\\\\n",
    "    0 \\text{        otherwise}\n",
    "  \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "for a certain threshold $\\theta$. It is not used in practice. \n",
    "\n",
    "- Sigmoid:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$ \n",
    "\n",
    "It is typically used in the last layer of a binary classification problem.\n",
    "\n",
    "- TanH - Hyperbolic tangent function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{2}{1 + e^{-2x}} - 1\n",
    "$$ \n",
    "\n",
    "It is typically used in the hidden layers. It is a sort of scaled and shifted sigmoid function. It is a good choice in hidden layer that takes values in [$-1, 1$]\n",
    "\n",
    "- ReLU function: \n",
    "\n",
    "$$\n",
    "f(x) = max(0, x)\n",
    "$$ \n",
    "\n",
    "It is the most popular choice in most of the networks. It will output 0 for negative values and the output for positive value. It doesn't look that different from a linear transformation but in fact it is non-linear and actually the most popular choice. **It is a good choice for activation function in hidden layers when you don't know what to use**.\n",
    "\n",
    "- Leaky ReLU function: \n",
    "\n",
    "$$\n",
    "f(x) =  \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    x & \\text{if  } x \\geq 0 \\\\\n",
    "    a \\cdot x \\text{        otherwise}\n",
    "  \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "It is an improved version of the ReLu function and it tries to solve the vanishing gradient problem. Here **a is typically very small**. With a normal ReLu, our negative values are 0 and thus the gradient layer in the backpropagation is also set to zero and this means that these weights will never be update so the neurones associated to this value won't learn anything. We also say that these neurons are dead. This is when you use the leaky ReLu. \n",
    "\n",
    "- Softmax: \n",
    "\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_{i=1}^n e^{y_i}}\n",
    "$$\n",
    "\n",
    "This function squashes the output to be between 0 and 1 so that we get probabilities. It is a good choice in the last layer of a multi-class classification problem.\n",
    "\n",
    "Let's jump into the code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## option 1: create nn Modules\n",
    "class NeuralNet2(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x): ## during this phase, we compute the prediction \n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "## option 2 (use activation functions directly in forward pass):\n",
    "    \n",
    "class NeuralNet2(nn.Module):\n",
    "\n",
    "    ## we only define the linear layers in the init function\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x): ## during this phase, we compute the prediction \n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
